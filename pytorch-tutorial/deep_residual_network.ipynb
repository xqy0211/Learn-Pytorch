{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 80\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 32, 32])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Pad(4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=False)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "example = enumerate(train_loader)\n",
    "i,(image,label) = next(example)\n",
    "print(image.size())\n",
    "print(label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    \"\"\"\n",
    "    3x3的same卷积(s=1时)\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                      stride=stride, padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualBlock(\n",
      "  (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        \"\"\"\n",
    "        describe:\n",
    "            Residual block(低层的残差块)，x = F(x) + x\n",
    "        input:\n",
    "            in_channels: 输入的channels数\n",
    "            out_channels: 输出channels数\n",
    "            stride: 步长，默认1\n",
    "            downsample: 是否下采样（相加的两个量维度是否相等）\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # 疑惑：为啥这里有stride，conv2没加，不都是stride=1吗\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # 将得到的值计算得到的值覆盖之前的值\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        describe:\n",
    "            x -> residual\n",
    "            x -> conv1 -> bn1 -> relu -> conv2 -> bn2\n",
    "            将上述两个叠加后（看是否维度一致：downsample=？）-> out\n",
    "            out -> relu -> 输出\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self .conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out   \n",
    "\n",
    "resblock = ResidualBlock(16, 32)\n",
    "print(resblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            block: 这里是ResidualBlock\n",
    "            layers: 一个一维list，存放每个layer对应的block数目，\n",
    "            如[2, 2, 2]对应ResNet18（不完全），layer1,2,3都有2个block\n",
    "            num_classes: 10类别(CIFAR10)\n",
    "            \n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16   # 输入到layer1的channel数为16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, out_channels=16, blocks=layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], stride=2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            block: 这里是ResidualBlock\n",
    "            out_channels: 输出的通道数\n",
    "            blocks: 当前layer的block数目，如layer2对应2\n",
    "            stride: 默认步长1\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            # 此种情况维度出现不一致, 通过3x3conv使得输入输出维度一致\n",
    "            # 以layer2为例：(stride=2)!=1且(in=16) != (out=32)\n",
    "            # 设置downsample为3x3的conv, stride=2, in=16, out=32\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []    # 定义layers 列表\n",
    "        # 列表中添加res block (16x32x32 -> 32x16x16)\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        # 更新in_channels值，将本次out_channels作为下次的in_channels\n",
    "        self.in_channels = out_channels\n",
    "        # 注意这里下标从1开始，以layer2举例，\n",
    "        # 这个for只append了一个res block（32x16x16 -> 32x16x16）\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        describe:\n",
    "            x(100,3,32,32) -> conv(100,16,32,32) -> bn -> relu -> \n",
    "            layer1(100,16,32,32) -> layer2(100,32,16,16) -> layer3(100,64,8,8) -> \n",
    "            avg_pool(100,64,1,1) -> resize(100,64) -> fc(100,10) -> 输出\n",
    "        \"\"\"\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c8f94de4cfa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mupdate_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [100/500], Loss: 1.5668\n",
      "Epoch: [1/5], Step: [200/500], Loss: 1.2881\n",
      "Epoch: [1/5], Step: [300/500], Loss: 1.3484\n",
      "Epoch: [1/5], Step: [400/500], Loss: 1.2415\n",
      "Epoch: [1/5], Step: [500/500], Loss: 1.0630\n",
      "Epoch: [2/5], Step: [100/500], Loss: 1.2061\n",
      "Epoch: [2/5], Step: [200/500], Loss: 1.0709\n",
      "Epoch: [2/5], Step: [300/500], Loss: 0.9323\n",
      "Epoch: [2/5], Step: [400/500], Loss: 0.7828\n",
      "Epoch: [2/5], Step: [500/500], Loss: 1.0241\n",
      "Epoch: [3/5], Step: [100/500], Loss: 1.0637\n",
      "Epoch: [3/5], Step: [200/500], Loss: 1.0324\n",
      "Epoch: [3/5], Step: [300/500], Loss: 0.8380\n",
      "Epoch: [3/5], Step: [400/500], Loss: 0.6729\n",
      "Epoch: [3/5], Step: [500/500], Loss: 0.7945\n",
      "Epoch: [4/5], Step: [100/500], Loss: 0.8276\n",
      "Epoch: [4/5], Step: [200/500], Loss: 0.7589\n",
      "Epoch: [4/5], Step: [300/500], Loss: 0.7063\n",
      "Epoch: [4/5], Step: [400/500], Loss: 0.7683\n",
      "Epoch: [4/5], Step: [500/500], Loss: 0.9856\n",
      "Epoch: [5/5], Step: [100/500], Loss: 0.5390\n",
      "Epoch: [5/5], Step: [200/500], Loss: 0.6316\n",
      "Epoch: [5/5], Step: [300/500], Loss: 0.7668\n",
      "Epoch: [5/5], Step: [400/500], Loss: 0.5722\n",
      "Epoch: [5/5], Step: [500/500], Loss: 0.5993\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epoches):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%100 == 0:\n",
    "            print(\"Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}\".format(\n",
    "                epoch+1, num_epoches, i+1, total_step, loss.item()))\n",
    "    \n",
    "    if (epoch+1)%20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 68.24%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _,predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Accuracy of the model on the test images: {}%'.format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet18.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('resnet18.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "test_example = enumerate(test_loader)\n",
    "_, (images, labels) = next(test_example)\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "    \n",
    "print(images.size())\n",
    "plt.imshow(images[1].permute(1,2,0))    # (3x32x32 -> 32x32x3)\n",
    "plt.show()\n",
    "print('label: {}'.format(classes[labels[1]]))\n",
    "    \n",
    "output = model(images)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print('predicted: {}'.format(classes[predicted[0]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: ship\n",
      "predicted: ship\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_gpu]",
   "language": "python",
   "name": "conda-env-pytorch_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}